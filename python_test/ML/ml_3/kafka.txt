
1）kafak简介
  kafka是一个分布式的消息系统，使用的是Scala开发的。支持c++ Java Python

        Python（网络爬虫，数据分析，机器学习，web,运维）  Spark

2）好处
解耦
冗余
峰值的处理能力：
可恢复性
异步通信

3）核心概念
 producer ： 特指消息的生产者
 consumer：特指消息的消费者
 consumer Group: 消费者组
 broker：kafka集群中的节点，起到缓存的作用
 topic: 数据的不同分类，类似于关系型数据库里面的表
 partition：topic的分组，一个topic可以分为多个partition
 Message：说白了就是数据。是通信的基本单位


 消息系统：
 类似于qq聊天，群聊是一对多，私聊就是一对一
 类似于群聊（一对多），这种模式叫做订阅模式（publish-subscribe） ps模式
 类似于私聊（一对一），这种模式叫做点对点的模式（point-to-point）p2p模式
 如果是p2p模式，当一个消费者消费了以后，其他人就消费不了了。
 如果是ps模式。
 A组：张三丰，张无忌
 B组：谢霆锋 lixiyuan
 张三丰消费了数据，张无忌不能消费。谢霆锋可以消费，lixiyuan就不能消费。

集群的搭建：
		         hadoop1                hadoop2                   hadoop3
zookeeper         是                      是                        是
broker            是                      是                        是


kafka.apache.org
版本：
0.7  0.8   0.9  1.0

kafka_2.11-0.8.2.2.tgz   tgz  .tar.gz

0.8.2.2指的是kafka的版本
2.11指的是Scala版本

免密码管理上的方便：
hadoop-daemon.sh
hadoop-daemons.sh
start-dfs.sh
start-all.sh(start-dfs.sh start-yarn.sh)
1）配置好主机（免密码）
2) 搭建好zookeeper集群
3）上传kafka，解压，修改配置文件就可以了。
  3.1  /usr/local/soft
  3.2 解压 tar -zxvf kafka_2.11-0.8.2.2.tgz
  3.3 重命名，配置环境变量   mv kafka_2.11-0.8.2.2  kafka


  修改配置文件（/home/hadoop/apps/kafka/config/server.properties--vi server.properties）：

  broker.id=0  标示符
  host.name=hadoop01 绑定的主机
  log.dirs=/home/hadoop/kafkatest/kafka-logs  数据保存的位置
  log.retention.hours=168  数据的保留时间
  zookeeper.connect=hadoop0:2181,hadoop02:2181,hadoop03:2181




测试：

启动服务的命令：

   未配置环境变量时：[/home/hadoop/apps/kafka/bin]$ ./kafka-server-start.sh
   配置环境变量时:kafka-server-start.sh ../config/server.properties

kafka-server-start.sh /opt/cloudera/parcels/KAFKA-3.1.0-1.3.1.0.p0.35/etc/kafka/conf.dist/server.properties
   后台启动命令： nohup  kafka-server-start.sh  ../config/server.properties   &

创建主题：
 kafka-topics.sh  --create --zookeeper manager:2181,namenode:2181,datanode:2181 --replication-factor 3 --partitions 1  --topic  luzimo
 查看当前有哪些主题：
  kafka-topics.sh --list --zookeeper  localhost:2181
 在Hadoop1上模拟，往kafka的aura主题里面发送数据
 然后Hadoop2上去消费这个数据

发送数据
 kafka-console-producer.sh  --broker-list manager:9092,namenode:9092,datanode:9092  --topic luzimo 
消费数据：
 kafka-console-consumer.sh --zookeeper manager:2181 --from-beginning --topic luzimo


kafka-console-producer.sh --broker-list manager:9092,namenode:9092,datanode:9092  --topic lujuhui
kafka-console-consumer.sh --zookeeper datanode:2181 --from-beginning --topic lujuhui
kafka-topics --delete --topic realtime --zookeeper manager:2181
-----------------------------------------------------------
1) VM
2) Vbox


---------------------------------------------------------

eclipse

IDEA

kafka: 消息（数据）系统
==========================================================
flume:日志（数据）收集系统
1.1 为什么会有收集这回事？
  数据往往存在业务系统里面。数据分析系统和业务系统在机器是独立在
  需要我们做数据分析的时候，把业务系统上面在数据收集过来

1.2 如何收集？
  

1.3 flume的组成
kafka: consumer  producer  broker  zookeeper
flume:只有一个角色  agent（代理）
agent有由三个部分组成：
1）source 类似于kafka里面的producer，用户采集数据，把采集到的数据发送到
    channel里面
2）channel  类似于kafka里面的broker，里面缓存数据，用于连接source和sink
3） sink   类似于kafka里面的consumer，从channel获取数据，将数据写到目的地。

1.4 如何使用flume

使用什么source
		source类型有很多：
		 1）hbase
		 2) kafka
		 3) 通过端口获取数据
		 4） 通过监控一个目录去获取
		 。。。。
使用什么channel
         channel类型也有
         1）文件系统里面
         2）内存里面
         3JDBC
         。。。。
使用什么sink
		 1）HDFS
		 2）目录 Linux
		 3）Hbase
		 4）flume
		 ......
我们使用flume就是需要我们根据自己的场景，挑选合适的
source ，channel ，sink 就可以了
--------------------------------------------------------
flume.apache.org
案例一：
1） 往Hadoop1 上面上传flume文件。
2）对文件进行解压
3）开始配置文件就可以了

=============================================================
1,首先要agent取一个名字，这个名字随便取，比如我们就叫a1
(1) /home/hadoop/apps/flume/conf/ vi luzimo.properties

a1.sources=r1
a1.sinks=s1 
a1.channels=c1
#spooldir  source
a1.sources.r1.type=spooldir
a1.sources.r1.spoolDir=/home/hadoop/hahaha
# channel
a1.channels.c1.type=memory
# sink
a1.sinks.s1.type=logger
#  
a1.sources.r1.channels=c1
a1.sinks.s1.channel=c1


(2)[hadoop@hadoop01 flume]$ bin/flume-ng agent --conf conf --conf-file conf/luzimo.properties --name a1 -Dflume.root.logger=INFO,console

flume启动测试命令：
1）cd /opt/cloudera/parcels/CDH-5.7.5-1.cdh5.7.5.p0.3
2) bin/flume-ng agent --conf conf --conf-file etc/flume-ng/conf.empty/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console

kafaka启动测试命令：
1）cd /opt/cloudera/parcels/KAFKA-3.1.0-1.3.1.0.p0.35/lib/kafka/bin
2) kafka-console-consumer.sh --zookeeper manager:2181,namenode:2181,datanode:2181 --from-beginning --topic realtime


(3)[hadoop@hadoop01 ~]$ vi c.txt

(4)[hadoop@hadoop01 ~]$ mv c.txt hahaha

=========================flume链接hdfs的配置=====start================================
-f 监控文件ID，一旦日志回滚将无法监测数据
-F 监控文件名

tail -F a.log 文件名
tail -f a.log  id
a.log
a3.log
a2.log
a1.log

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /usr/local/soft/flume/flume_dir/access.log //监控日志命令

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop1:8020/aura/%y-%m-%d/%H%M/  //按照时间创建文件夹
a1.sinks.k1.hdfs.filePrefix = hello-   //文件夹以hello-开头
a1.sinks.k1.hdfs.round = true  //日志目录是否回滚-1
a1.sinks.k1.hdfs.roundValue = 10  //十分钟-2
a1.sinks.k1.hdfs.roundUnit = minute //-3 ：1+2+3 ： 每十分钟对日志目录进行回滚一次
a1.sinks.k1.hdfs.rollInterval = 3
a1.sinks.k1.hdfs.rollSize = 20
a1.sinks.k1.hdfs.rollCount = 5
a1.sinks.k1.hdfs.batchSize = 1
a1.sinks.k1.hdfs.useLocalTimeStamp = true  //按照本地获取的时间建立文件夹
a1.sinks.k1.hdfs.fileType = DataStream //文件类型

# Use a channel which buffers events in memory
a1.channels.c1.type = memory

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


=======================flume链接hdfs的配置=====end========================


